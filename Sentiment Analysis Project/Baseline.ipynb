{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline.ipynb","provenance":[{"file_id":"1K7Kg0yUaf0dUlRLZZ7IpSLk-_WT9Yik6","timestamp":1588146108957},{"file_id":"10P-65DyIsP9G-YRF9Y4HWBFDOIl8lexD","timestamp":1583787959570},{"file_id":"1UlKNvAxZ2HRO6PYwo9zeZZouD7eQtKYY","timestamp":1583269302856},{"file_id":"1tm76NjY9qR153YyZIEFPrgTnrKEkT7ej","timestamp":1583267747863}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"B-ldN7hW5DkP","colab_type":"text"},"source":["**Importing relevant modules**"]},{"cell_type":"code","metadata":{"id":"crr1qghm4e7T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1588149674472,"user_tz":240,"elapsed":2943,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"0b8a5179-7bd4-4d58-ed77-ec329fd5f856"},"source":["from textblob import TextBlob\n","import unicodedata as uni\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import nltk\n","import re\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import cross_val_score\n","#from sklearn import cross_validation\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.model_selection import StratifiedKFold\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(\"english\")\n","from nltk.stem.wordnet import WordNetLemmatizer \n","#stemmer = WordNetLemmatizer()\n","import csv\n","from scipy.stats import spearmanr\n","import sklearn.feature_extraction.text as text\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import xgboost as xgb\n","from xgboost.sklearn import XGBClassifier\n","import string\n","\n","import unicodedata as uni\n","!pip install emoji\n","import emoji\n","\n","from nltk.tokenize import TweetTokenizer\n","from nltk.corpus import stopwords\n","\n","\n","\n","import math\n","import gensim.models as gs\n","import pickle as pk\n","import sklearn.metrics as met\n","import scipy.stats as stats\n","import numpy as np"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RM9qOx9Z5gbI","colab_type":"text"},"source":["**Importing the Datasets**"]},{"cell_type":"code","metadata":{"id":"AKPTW5O-1ruC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1588149678201,"user_tz":240,"elapsed":319,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"0abeb8e5-8f2f-463a-ec19-40df8819a6d4"},"source":["path1 = \"Final_Emojis - emotions1.csv\"\n","df1 = pd.read_csv(path1)\n","df1.keys()\n","\n","path2 = \"Dataset_With_Emoji.csv\"\n","df2 = pd.read_csv(path2)\n","df2.keys()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Unnamed: 0', 'Tweet', 'Affect Dimension', 'Unnamed: 2'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"p_myAufA6ENl","colab_type":"text"},"source":["**Visualize first dataset**"]},{"cell_type":"code","metadata":{"id":"q-FxC6kR4cn2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1588149680020,"user_tz":240,"elapsed":249,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"19d602a9-9258-4559-e87b-c728281e3249"},"source":["df1.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Tweet</th>\n","      <th>Affect Dimension</th>\n","      <th>Unnamed: 2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ðŸ˜¡@chetan_bhagat ðŸ˜¤ðŸ˜¤ Right to speech... Its bitt...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Low key Dro f*cked Molly like he had a point t...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@TeleTransOne @eleganza_lily @FemFataleXX  Tha...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@F3Alpha #F3Roswell #Preblast\\nI've ðŸ˜¤ðŸ˜¤been han...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Did they offend us and they want it to sound n...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               Tweet  ... Unnamed: 2\n","0  ðŸ˜¡@chetan_bhagat ðŸ˜¤ðŸ˜¤ Right to speech... Its bitt...  ...        NaN\n","1  Low key Dro f*cked Molly like he had a point t...  ...        NaN\n","2  @TeleTransOne @eleganza_lily @FemFataleXX  Tha...  ...        NaN\n","3  @F3Alpha #F3Roswell #Preblast\\nI've ðŸ˜¤ðŸ˜¤been han...  ...        NaN\n","4  Did they offend us and they want it to sound n...  ...        NaN\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"rNZOnxuf6ZKp","colab_type":"text"},"source":["**Visualize second dataset**"]},{"cell_type":"code","metadata":{"id":"gK9uEpZ06Bq0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1588149681829,"user_tz":240,"elapsed":232,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"a4dfc400-994b-410a-becb-dd0ef62d7f78"},"source":["df2.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Tweet</th>\n","      <th>Affect Dimension</th>\n","      <th>Unnamed: 2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>pouting face @chetan_bhagat  face with steam ...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Low key Dro f*cked Molly like he had a point t...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>@TeleTransOne @eleganza_lily @FemFataleXX  Tha...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>@F3Alpha #F3Roswell #Preblast\\nI've  face with...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Did they offend us and they want it to sound n...</td>\n","      <td>anger</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ... Unnamed: 2\n","0           0  ...        NaN\n","1           1  ...        NaN\n","2           2  ...        NaN\n","3           3  ...        NaN\n","4           4  ...        NaN\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"kvFPUyGry3-W","colab_type":"code","colab":{}},"source":["#In case we want to eliminate Neutral statements\n","#df1 = df1.drop(df1[df1['Affect Dimension']=='neutral'].index, axis = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QO9mwvtafN93","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1588149683138,"user_tz":240,"elapsed":124,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"1e8e2476-3da9-4527-a655-5be4e846ece5"},"source":["#Code used to convert Emojis to Text\n","\"\"\"def is_emoji(s):\n","    count = 0\n","    for emoji1 in emoji.UNICODE_EMOJI:\n","        count += s.count(emoji1)\n","        if count > 1:\n","            return False\n","    return bool(count)\n","\n","def emoji2text(title):\n","    stemmed = []\n","    tokenized = []\n","    series = []\n","    print(type(series))\n","    for i in title:\n","      #print(type(title))\n","      for c in i:\n","        if is_emoji(c) == True:\n","          word1 = emoji.demojize(c)\n","          word1 = word1.replace('_', ' ')\n","          word1 = word1.replace(':', ' ')\n","          #word2 = \" \".join(word1)\n","          i = i.replace(c, word1)\n","      series.append(i)\n","    return series\n","\n","df1.Tweet = emoji2text(df1.Tweet)\n","tweets = (df1['Tweet'])\n","df1.head()\n","labels = df1['Affect Dimension']\n","df1.to_csv('Dataset_With_Emoji.csv')\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def is_emoji(s):\\n    count = 0\\n    for emoji1 in emoji.UNICODE_EMOJI:\\n        count += s.count(emoji1)\\n        if count > 1:\\n            return False\\n    return bool(count)\\n\\ndef emoji2text(title):\\n    stemmed = []\\n    tokenized = []\\n    series = []\\n    print(type(series))\\n    for i in title:\\n      #print(type(title))\\n      for c in i:\\n        if is_emoji(c) == True:\\n          word1 = emoji.demojize(c)\\n          word1 = word1.replace(\\'_\\', \\' \\')\\n          word1 = word1.replace(\\':\\', \\' \\')\\n          #word2 = \" \".join(word1)\\n          i = i.replace(c, word1)\\n      series.append(i)\\n    return series\\n\\ndf1.Tweet = emoji2text(df1.Tweet)\\ntweets = (df1[\\'Tweet\\'])\\ndf1.head()\\nlabels = df1[\\'Affect Dimension\\']\\ndf1.to_csv(\\'Dataset_With_Emoji.csv\\')'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"lSdd6vpD7UXC","colab_type":"text"},"source":["**Tokenization if emojis in dataset are not converted to text, and we need to take text into consideration (takes a long time to run, which is why we already converted emojis in our dataset for repeated use)**"]},{"cell_type":"code","metadata":{"id":"Jm3tAKQ45M-_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1588149684784,"user_tz":240,"elapsed":208,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"21938c4c-8eda-4551-f702-b8072c7293b1"},"source":["#main function\n","\"\"\"def ts(title):\n","    stemmed = []\n","    tokenized = []\n","    for i1, i in enumerate(title):\n","        print(i1)\n","        stemmed1 = tokenstem(i) \n","        tokenized1 = token(i)\n","        stemmed.extend(stemmed1)\n","        tokenized.extend(tokenized1)\n","    return stemmed, tokenized\n","\n","#side functions\n","def tokenstem(text):\n","    words1 = []\n","    tk = nltk.TweetTokenizer()\n","    #words = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)] #tokenize sentences then word\n","    try:\n","      words = [word for sent in nltk.sent_tokenize(text) for word in tk.tokenize(sent)]\n","    except:\n","      print(text)\n","    \n","    #print(words)\n","    words11=words.copy()\n","    idx=0\n","    i2=0\n","    n=0\n","    #print(words11)\n","    for i1, word in enumerate(words11):\n","      \n","      #print(word)\n","      if i2!=0 and n!=0:\n","        words.insert(idx, word1)\n","        idx+=1\n","        n=0\n","      if i2==0 and i1!=0:\n","        idx+=1\n","      i2=0\n","      m=0\n","      n=0\n","      #print(idx)\n","      for c in word:\n","      #c for c in str if c in emoji.UNICODE_EMOJI\n","        #print(c)\n","        if is_emoji(c) == True:\n","          i2+=1\n","          if m==0:\n","            try:\n","              del(words[idx])\n","            except:\n","              print(words, idx)\n","          if n!=0:\n","            words.insert(idx, word1)\n","            idx+=1\n","            n=0\n","          word1 = emoji.demojize(c)\n","          word1 = word1.replace('_', ' ')\n","          word1 = word1.replace(':', '')\n","          word1 = nltk.word_tokenize((word1))\n","          for i in range(len(word1)):\n","            words.insert(idx, word1[i])\n","            idx+=1\n","          m+=1\n","        else:\n","          if n==0:\n","            word1 = c\n","          else:\n","            word1=word1+c\n","          n+=1\n","    for token in words:\n","        if re.search('[a-zA-Z]', token): #check if it is a word\n","            words1.append(token)\n","    #stems = [stemmer.lemmatize(t) for t in words1]\n","    stems = [stemmer.stem(t) for t in words1]\n","    return stems\n","\n","def token(text):\n","    words2 = []\n","    tk = nltk.TweetTokenizer()\n","    #words = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","    try:\n","      words = [word for sent in nltk.sent_tokenize(text) for word in tk.tokenize(sent)]\n","    except:\n","      print(text)\n","    words11=words.copy()\n","    idx=0\n","    i2=0\n","    n=0\n","    for i1, word in enumerate(words11):\n","      #print(word)\n","      if i2!=0 and n!=0:\n","        words.insert(idx, word1)\n","        idx+=1\n","        n=0\n","      if i2==0 and i1!=0:\n","        idx+=1\n","      i2=0\n","      m=0\n","      n=0\n","\n","\n","      for c in word:\n","      #c for c in str if c in emoji.UNICODE_EMOJI\n","        if is_emoji(c) == True:\n","          i2+=1\n","          if m==0:\n","            try:\n","              del(words[idx])\n","            except:\n","              print(words, idx)\n","          if n!=0:\n","            words.insert(idx, word1)\n","            idx+=1\n","            n=0\n","          word1 = emoji.demojize(c)\n","          word1 = word1.replace('_', ' ')\n","          word1 = word1.replace(':', '')\n","          word1 = nltk.word_tokenize((word1))\n","          for i in range(len(word1)):\n","            words.insert(idx, word1[i])\n","            #print(i)\n","            idx+=1\n","          m+=1\n","        else:\n","          if n==0:\n","            word1 = c\n","          else:\n","            word1=word1+c\n","          n+=1\n","    for token in words:\n","        if re.search('[a-zA-Z]', token):\n","            words2.append(token)\n","    return words2\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"def ts(title):\\n    stemmed = []\\n    tokenized = []\\n    for i1, i in enumerate(title):\\n        print(i1)\\n        stemmed1 = tokenstem(i) \\n        tokenized1 = token(i)\\n        stemmed.extend(stemmed1)\\n        tokenized.extend(tokenized1)\\n    return stemmed, tokenized\\n\\n#side functions\\ndef tokenstem(text):\\n    words1 = []\\n    tk = nltk.TweetTokenizer()\\n    #words = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)] #tokenize sentences then word\\n    try:\\n      words = [word for sent in nltk.sent_tokenize(text) for word in tk.tokenize(sent)]\\n    except:\\n      print(text)\\n    \\n    #print(words)\\n    words11=words.copy()\\n    idx=0\\n    i2=0\\n    n=0\\n    #print(words11)\\n    for i1, word in enumerate(words11):\\n      \\n      #print(word)\\n      if i2!=0 and n!=0:\\n        words.insert(idx, word1)\\n        idx+=1\\n        n=0\\n      if i2==0 and i1!=0:\\n        idx+=1\\n      i2=0\\n      m=0\\n      n=0\\n      #print(idx)\\n      for c in word:\\n      #c for c in str if c in emoji.UNICODE_EMOJI\\n        #print(c)\\n        if is_emoji(c) == True:\\n          i2+=1\\n          if m==0:\\n            try:\\n              del(words[idx])\\n            except:\\n              print(words, idx)\\n          if n!=0:\\n            words.insert(idx, word1)\\n            idx+=1\\n            n=0\\n          word1 = emoji.demojize(c)\\n          word1 = word1.replace('_', ' ')\\n          word1 = word1.replace(':', '')\\n          word1 = nltk.word_tokenize((word1))\\n          for i in range(len(word1)):\\n            words.insert(idx, word1[i])\\n            idx+=1\\n          m+=1\\n        else:\\n          if n==0:\\n            word1 = c\\n          else:\\n            word1=word1+c\\n          n+=1\\n    for token in words:\\n        if re.search('[a-zA-Z]', token): #check if it is a word\\n            words1.append(token)\\n    #stems = [stemmer.lemmatize(t) for t in words1]\\n    stems = [stemmer.stem(t) for t in words1]\\n    return stems\\n\\ndef token(text):\\n    words2 = []\\n    tk = nltk.TweetTokenizer()\\n    #words = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\\n    try:\\n      words = [word for sent in nltk.sent_tokenize(text) for word in tk.tokenize(sent)]\\n    except:\\n      print(text)\\n    words11=words.copy()\\n    idx=0\\n    i2=0\\n    n=0\\n    for i1, word in enumerate(words11):\\n      #print(word)\\n      if i2!=0 and n!=0:\\n        words.insert(idx, word1)\\n        idx+=1\\n        n=0\\n      if i2==0 and i1!=0:\\n        idx+=1\\n      i2=0\\n      m=0\\n      n=0\\n\\n\\n      for c in word:\\n      #c for c in str if c in emoji.UNICODE_EMOJI\\n        if is_emoji(c) == True:\\n          i2+=1\\n          if m==0:\\n            try:\\n              del(words[idx])\\n            except:\\n              print(words, idx)\\n          if n!=0:\\n            words.insert(idx, word1)\\n            idx+=1\\n            n=0\\n          word1 = emoji.demojize(c)\\n          word1 = word1.replace('_', ' ')\\n          word1 = word1.replace(':', '')\\n          word1 = nltk.word_tokenize((word1))\\n          for i in range(len(word1)):\\n            words.insert(idx, word1[i])\\n            #print(i)\\n            idx+=1\\n          m+=1\\n        else:\\n          if n==0:\\n            word1 = c\\n          else:\\n            word1=word1+c\\n          n+=1\\n    for token in words:\\n        if re.search('[a-zA-Z]', token):\\n            words2.append(token)\\n    return words2\""]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Uh_GtXOguinz","colab_type":"text"},"source":["**For tokenization and stemming only**"]},{"cell_type":"code","metadata":{"id":"URd_Tn3TuljI","colab_type":"code","colab":{}},"source":["def ts(title):\n","    stemmed = []\n","    tokenized = []\n","    for i in title:\n","        stemmed1 = tokenstem(i) \n","        tokenized1 = token(i)\n","        stemmed.extend(stemmed1)\n","        tokenized.extend(tokenized1)\n","    return stemmed, tokenized\n","\n","#side functions\n","def tokenstem(text):\n","    words1 = []\n","    tk = nltk.TweetTokenizer()\n","    #words = [word for sent in sent1 for word in nltk.word_tokenize(sent)] #tokenize sentences then word\n","    try:\n","      words = [word for sent in nltk.sent_tokenize(text) for word in tk.tokenize(sent)]\n","    except:\n","      print(words, text)\n","    #words = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)] #tokenize sentences then word\n","    for token in words:\n","        if re.search('[a-zA-Z]', token): #check if it is a word\n","            words1.append(token)\n","    #stems = [stemmer.lemmatize(t) for t in words1]\n","    stems = [stemmer.stem(t) for t in words1]\n","    return stems\n","\n","def token(text):\n","    words2 = []\n","    tk = nltk.TweetTokenizer()\n","    #words = [word for sent in sent1 for word in nltk.word_tokenize(sent)] #tokenize sentences then word\n","    words = [word for sent in nltk.sent_tokenize(text) for word in tk.tokenize(sent)]\n","    #words = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)] #tokenize sentences then word\n","    for token in words:\n","        if re.search('[a-zA-Z]', token):\n","            words2.append(token)\n","    return words2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jg7sCHu49oI8","colab_type":"text"},"source":["Using TFIDF Vectorizer"]},{"cell_type":"code","metadata":{"id":"OCxN9Zuq5jC7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1588149708557,"user_tz":240,"elapsed":19881,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"48a782dc-f48d-498d-a3a0-755154ae9acb"},"source":["#On Dataset 1\n","titles = df1.Tweet.str.lower() #to make it lower case\n","stemmedop, tokenizedop = ts(titles)\n","#To remove repitions for better output\n","words = zip(stemmedop, tokenizedop) \n","words = list(set(words))\n","stemmed2, tokenized2 = zip(*words)\n","merged = pd.DataFrame({'words': tokenized2}, index = stemmed2)  #to put words under a specific stem\n","#Using NLTK to get stopwords to remove it from our list\n","stopwords = nltk.corpus.stopwords.words('english')\n","stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n","# tf-idf vectorizer\n","tfidf_vectorizer = TfidfVectorizer(min_df =10**-3 ,analyzer = 'word', max_features=len(set(stemmed2)), stop_words=stop_words, tokenizer=tokenstem, ngram_range=(1,3))\n","tfidf1 = tfidf_vectorizer.fit_transform(titles)\n","\n","#On Dataset 2\n","titles = df2.Tweet.str.lower() #to make it lower case\n","stemmedop, tokenizedop = ts(titles)\n","#To remove repitions for better output\n","words = zip(stemmedop, tokenizedop) \n","words = list(set(words))\n","stemmed2, tokenized2 = zip(*words)\n","merged = pd.DataFrame({'words': tokenized2}, index = stemmed2)  #to put words under a specific stem\n","#Using NLTK to get stopwords to remove it from our list\n","stopwords = nltk.corpus.stopwords.words('english')\n","stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n","# tf-idf vectorizer\n","tfidf_vectorizer = TfidfVectorizer(min_df =10**-3 ,analyzer = 'word', max_features=len(set(stemmed2)), stop_words=stop_words, tokenizer=tokenstem, ngram_range=(1,3))\n","tfidf2 = tfidf_vectorizer.fit_transform(titles)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', \"should'v\", 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', \"should'v\", 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"wFBIQaq-CadF","colab_type":"text"},"source":["**Initializing Stratified K-Fold Cross Validation**"]},{"cell_type":"code","metadata":{"id":"DtwPD0j4560Z","colab_type":"code","colab":{}},"source":["a=np.array(df1['Affect Dimension'])\n","kf = StratifiedKFold(n_splits=10, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wtuvYMj9xUC","colab_type":"text"},"source":["**Applying Linear SVM**"]},{"cell_type":"code","metadata":{"id":"YyHjyNf77sVq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1588149716023,"user_tz":240,"elapsed":2567,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"6c57b78e-88b1-4a5d-d93d-f415e3d51087"},"source":["#Dataset1\n","clf = LinearSVC()\n","scores=(cross_val_score(clf, tfidf1, a, cv=kf))\n","print(\"Accuracy without Emoji: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))\n","\n","#Dataset2\n","clf = LinearSVC()\n","scores=(cross_val_score(clf, tfidf2, a, cv=kf))\n","print(\"Accuracy with Emoji: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy without Emoji: 0.682812 (+/- 0.0250879)\n","Accuracy with Emoji: 0.830136 (+/- 0.0107389)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GWIQt-x8CNSz","colab_type":"text"},"source":["**Applying Logistic Regression**"]},{"cell_type":"code","metadata":{"id":"m9iAoENH7wh9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1588149726091,"user_tz":240,"elapsed":7163,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"de5e0d86-0502-4866-aad7-c2ab0a1ff823"},"source":["#Dataset 1\n","clf = LogisticRegression(max_iter=1000)\n","scores=(cross_val_score(clf, tfidf1, a, cv=kf, n_jobs=-1))\n","print(\"Accuracy without Emoji: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))\n","\n","#Dataset 2\n","clf = LogisticRegression(max_iter=1000)\n","scores=(cross_val_score(clf, tfidf2, a, cv=kf, n_jobs=-1))\n","print(\"Accuracy with Emoji: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy without Emoji: 0.67751 (+/- 0.0228729)\n","Accuracy with Emoji: 0.834425 (+/- 0.0260759)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qGh0KPH6EDWB","colab_type":"text"},"source":["**Applying Multinomial NaÃ¯ve Bias**"]},{"cell_type":"code","metadata":{"id":"cPTyBEQb9AWw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1588149730018,"user_tz":240,"elapsed":1052,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"9fd2a4ac-b2ae-4c5d-b30f-7aa72227d1dc"},"source":["#Dataset 1\n","clf = MultinomialNB()\n","scores=(cross_val_score(clf, tfidf1, a, cv=kf, n_jobs=-1))\n","print(\"Accuracy: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))\n","\n","#Dataset 2\n","clf = MultinomialNB()\n","scores=(cross_val_score(clf, tfidf2, a, cv=kf, n_jobs=-1))\n","print(\"Accuracy: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.673997 (+/- 0.0151847)\n","Accuracy: 0.834037 (+/- 0.0189582)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8QwWeX43EVv5","colab_type":"text"},"source":["**Applying Random Forest Classifier**"]},{"cell_type":"code","metadata":{"id":"4AWx378E_SKR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1588149759058,"user_tz":240,"elapsed":26714,"user":{"displayName":"Amogh Rane","photoUrl":"","userId":"11424382904643377417"}},"outputId":"2c70ae0c-ddd8-438e-a85a-cbef0e8645ef"},"source":["#Dataset 1\n","clf = RandomForestClassifier(n_jobs = -1, max_features = 'sqrt', n_estimators = 35, oob_score = True)\n","scores=(cross_val_score(clf, tfidf1, a, cv=kf))\n","print(\"Accuracy: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))\n","\n","#Dataset 2\n","clf = RandomForestClassifier(n_jobs = -1, max_features = 'sqrt', n_estimators = 35, oob_score = True)\n","scores=(cross_val_score(clf, tfidf2, a, cv=kf))\n","print(\"Accuracy: %g (+/- %g)\" % (scores.mean(), scores.std() * 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.658086 (+/- 0.0264358)\n","Accuracy: 0.817814 (+/- 0.0232699)\n"],"name":"stdout"}]}]}